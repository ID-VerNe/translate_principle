# -*- coding: utf-8 -*-

import os
import json
import argparse
import asyncio
import logging
import hashlib
from typing import List, Dict
from tqdm import tqdm

# åœ¨å®šä¹‰å’Œä¿®æ”¹é…ç½®å‰ï¼Œå…ˆå¯¼å…¥å®ƒä»¬
from core.config import TranslationConfig
from core.srt_utils import parse_srt, format_srt_block
from core.translation_pipeline import extract_global_terms, process_literal_stage, process_polish_stage
from core.glossary_manager import glossary_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("translation.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def save_checkpoint(srt_file: str, progress_file: str, blocks: List[Dict], progress_data: Dict, bilingual_output: bool = False, last_context: str = ""):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨ srt_utils ç»Ÿä¸€æ ¼å¼åŒ–ã€‚
    """
    if not blocks:
        return

    output_block_index = progress_data.get('output_block_index', 1)

    with open(srt_file, 'a', encoding='utf-8') as f:
        for b in blocks:
            if bilingual_output:
                # å—åˆ†ç¦»æ¨¡å¼
                f.write(format_srt_block(output_block_index, b['timestamp'], b['original']))
                output_block_index += 1
                f.write(format_srt_block(output_block_index, b['timestamp'], b['polished']))
                output_block_index += 1
            else:
                f.write(format_srt_block(output_block_index, b['timestamp'], b['polished']))
                output_block_index += 1
    
    progress_data['output_block_index'] = output_block_index
    progress_data['last_context'] = last_context
    last_idx = int(blocks[-1]['index'])
    progress_data["last_index"] = last_idx
    processed_set = set(progress_data.get("processed_indices", []))
    processed_set.update(b['index'] for b in blocks)
    progress_data["processed_indices"] = sorted(list(processed_set), key=int)

    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)

def load_progress(progress_file: str) -> Dict:
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {"last_index": 0, "processed_indices": []}

async def run_translation(args):
    """æ‰§è¡Œç¿»è¯‘æµç¨‹çš„æ ¸å¿ƒé€»è¾‘"""
    
    # --- 0. åˆå§‹åŒ–é…ç½®ä¸è¯­æ–™åº“ ---
    config = TranslationConfig(
        api_key=args.api_key,
        api_url=args.api_url,
        model_name=args.model_name,
        temp_terms=args.temp_terms,
        temp_literal=args.temp_literal,
        temp_polish=args.temp_polish,
        max_concurrent_requests=args.max_concurrent
    )
    glossary_manager.initialize()

    # --- 0.1 åŠ¨æ€å¤„ç†ç¼“å­˜è·¯å¾„ ---
    cache_dir = ".cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
    
    input_filename = os.path.basename(args.input_file)
    file_hash = hashlib.md5(input_filename.encode('utf-8')).hexdigest()
    
    glossary_cache_file = getattr(args, 'glossary_cache_file', None)
    if glossary_cache_file is None:
        glossary_cache_file = os.path.join(cache_dir, f"glossary_{file_hash}.json")
        
    progress_file = getattr(args, 'progress_file', None)
    if progress_file is None:
        progress_file = os.path.join(cache_dir, f"progress_{file_hash}.json")

    # --- 1. åŠ è½½ SRT ---
    blocks = parse_srt(args.input_file)
    if not blocks:
        logger.error(f"æ— æ³•ä» {args.input_file} åŠ è½½ä»»ä½•å­—å¹•å—ã€‚")
        return
    logger.info(f"æˆåŠŸåŠ è½½åŸæ–‡: {len(blocks)} å—")

    # --- 2. æ„å»ºå½“å‰ä»»åŠ¡çš„æ··åˆæœ¯è¯­è¡¨ ---
    current_glossary = {}
    if os.path.exists(glossary_cache_file):
        try:
            with open(glossary_cache_file, 'r', encoding='utf-8') as f:
                current_glossary = json.load(f)
            logger.info(f"åŠ è½½ä»»åŠ¡ç¼“å­˜æœ¯è¯­è¡¨: {len(current_glossary)} æ¡")
        except:
            pass
            
    if not current_glossary:
        logger.info("å¼€å§‹æå–å…¨å±€æœ¯è¯­è¡¨...")
        current_glossary = await extract_global_terms(config, blocks)
        with open(glossary_cache_file, 'w', encoding='utf-8') as f:
            json.dump(current_glossary, f, ensure_ascii=False, indent=2)
        logger.info(f"æœ¯è¯­è¡¨å·²ä¿å­˜è‡³: {glossary_cache_file}")

    # --- æ˜¾çœ¼æç¤ºç”¨æˆ·æœ¯è¯­è¡¨ä½ç½® ---
    print("\n" + "="*60)
    print(f"ğŸ“‹ ã€å½“å‰ç”Ÿæ•ˆçš„æœ¯è¯­è¡¨ã€‘")
    print(f"   è·¯å¾„: {os.path.abspath(glossary_cache_file)}")
    print(f"   æç¤º: è‹¥éœ€äººå·¥ä¿®æ­£æœ¯è¯­ï¼Œè¯·ç¼–è¾‘æ­¤æ–‡ä»¶åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
    print("="*60 + "\n")

    # --- 3. æ¢å¤è¿›åº¦ ---
    progress = load_progress(progress_file)
    processed_indices = set(progress.get("processed_indices", []))
    remaining_blocks = [b for b in blocks if b['index'] not in processed_indices]

    if not remaining_blocks:
        logger.info("æ‰€æœ‰å­—å¹•å—éƒ½å·²å¤„ç†å®Œæ¯•ã€‚")
        return

    if not processed_indices:
        open(args.output_file, 'w').close()
        progress['output_block_index'] = 1 
    progress.setdefault('output_block_index', 1)

    logger.info(f"å¼€å§‹å¤„ç†ï¼Œå‰©ä½™ {len(remaining_blocks)} å—...")

    # ä»è¿›åº¦æ–‡ä»¶ä¸­æ¢å¤ä¸Šä¸‹æ–‡
    previous_context_str = progress.get('last_context', "")

    # --- 4. å‡†å¤‡æ‰¹æ¬¡åˆ—è¡¨ ---
    batches = []
    for i in range(0, len(remaining_blocks), args.batch_size):
        batches.append(remaining_blocks[i: i + args.batch_size])

    # --- 5. æµæ°´çº¿å¹¶è¡Œå¤„ç† ---
    literal_tasks = {}
    PREFETCH_WINDOW = 3 
    total_batches = len(batches)

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ€»è¿›åº¦
    pbar = tqdm(total=total_batches, desc="ç¿»è¯‘è¿›åº¦", unit="batch")

    for i, batch in enumerate(batches):
        start_id, end_id = batch[0]['index'], batch[-1]['index']

        # A. å¯åŠ¨é¢„å–ä»»åŠ¡
        for j in range(i, min(i + PREFETCH_WINDOW + 1, total_batches)):
            if j not in literal_tasks:
                task = asyncio.create_task(process_literal_stage(batches[j], config, current_glossary))
                literal_tasks[j] = task

        # B. è·å–ç›´è¯‘ç»“æœ
        literal_map, glossary_text = await literal_tasks[i]
        del literal_tasks[i]

        # C. å‡†å¤‡ä¸‹æ–‡ (Future Context)
        future_context_str = ""
        if i + 1 < total_batches:
            # å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„å…¨éƒ¨åŸæ–‡
            future_blocks = batches[i+1]
            future_context_str = "\n".join([f"- {b['content']}" for b in future_blocks])

        # D. æ‰§è¡Œæ¶¦è‰²é˜¶æ®µ
        final_blocks = await process_polish_stage(
            batch, config, literal_map, glossary_text, 
            previous_context=previous_context_str,
            future_context=future_context_str
        )
        
        if final_blocks:
            # æ›´æ–°ä¸Šæ–‡ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™å½“å‰æ‰¹æ¬¡çš„å…¨éƒ¨ç¿»è¯‘ç»“æœä¾›ä¸‹ä¸€æ‰¹æ¬¡å‚è€ƒï¼‰
            previous_context_str = "\n".join(
                [f"- {b['original']} -> {b['polished']}" for b in final_blocks]
            )

            save_checkpoint(args.output_file, progress_file, final_blocks, progress, bilingual_output=args.bilingual, last_context=previous_context_str)
            pbar.update(1)
            # tqdm.write å¯ä»¥åœ¨ä¸ç ´åè¿›åº¦æ¡çš„æƒ…å†µä¸‹æ‰“å°ä¿¡æ¯
            tqdm.write(f"  âœ… æ‰¹æ¬¡ {i+1} (ID {start_id}-{end_id}) å¤„ç†å®Œæˆã€‚")
        else:
            logger.warning(f"æ‰¹æ¬¡ {i+1} æœªç”Ÿæˆä»»ä½•å†…å®¹ã€‚")

    pbar.close()
    logger.info("ç¿»è¯‘ä»»åŠ¡åœ†æ»¡å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SRT æ™ºèƒ½ç¿»è¯‘å·¥å…· (æ¶æ„ä¼˜åŒ–ç‰ˆ)")

    # --- æ–‡ä»¶ä¸è·¯å¾„å‚æ•° ---
    parser.add_argument('-i', '--input-file', type=str, default='å®˜æ–¹è‹±æ–‡.srt', help='è¾“å…¥SRTæ–‡ä»¶')
    parser.add_argument('-o', '--output-file', type=str, default='å®˜æ–¹è‹±æ–‡_output.srt', help='è¾“å‡ºSRTæ–‡ä»¶')
    parser.add_argument('--progress-file', type=str, default=None, help='è¿›åº¦æ–‡ä»¶')
    parser.add_argument('--glossary-cache-file', type=str, default=None, help='æœ¯è¯­ç¼“å­˜')
    
    # --- è¿è¡Œå‚æ•° ---
    defaults = TranslationConfig()
    parser.add_argument('--batch-size', type=int, default=defaults.batch_size, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max-concurrent', type=int, default=defaults.max_concurrent_requests, help='æœ€å¤§å¹¶å‘è¯·æ±‚æ•°')

    parser.add_argument('--bilingual', dest='bilingual', action='store_true', help='å¼€å¯åŒè¯­')
    parser.add_argument('--no-bilingual', dest='bilingual', action='store_false', help='ä»…ä¸­æ–‡')
    parser.set_defaults(bilingual=True)

    # --- API ä¸æ¨¡å‹å‚æ•° ---
    parser.add_argument('--api-key', type=str, default=defaults.api_key, help='API Key')
    parser.add_argument('--api-url', type=str, default=defaults.api_url, help='API URL')
    parser.add_argument('--model-name', type=str, default=defaults.model_name, help='æ¨¡å‹åç§°')
    
    # --- æ¸©åº¦å‚æ•° ---
    parser.add_argument('--temp-terms', type=float, default=defaults.temp_terms, help='æœ¯è¯­æå–æ¸©åº¦')
    parser.add_argument('--temp-literal', type=float, default=defaults.temp_literal, help='ç›´è¯‘æ¸©åº¦')
    parser.add_argument('--temp-polish', type=float, default=defaults.temp_polish, help='æ¶¦è‰²æ¸©åº¦')

    args = parser.parse_args()

    # å¯åŠ¨å¼‚æ­¥ä¸»é€»è¾‘
    asyncio.run(run_translation(args))

if __name__ == "__main__":
    main()